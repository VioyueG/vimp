---
title: "Introduction to npvi"
author: "Brian D. Williamson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to npvi}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
references:
- id: hastie1990
  title: Generalized Additive Models
  author:
  - family: Hastie
    given: TJ
  - family: Tibshirani
    given: RJ
  #container-title: CRC Press
  volume: 43
  #URL: 'http://dx.doi.org/10.1038/nmat3283'
  #DOI: 10.1038/nmat3283
  #issue: 4
  publisher: CRC Press
  #page: 261-263
  type: book
  issued:
    year: 1990
    #month: 3
- id: vanderlaan2007
  title: Super learner
  author: 
  - family: van der Laan
    given: MJ
  - family: Polley
    given: EC
  - family: Hubbard
    given: AE
  volume: 6
  publisher: Statistical Applications in Genetics and Molecular Biology
  type: article-journal
  issued:
   year: 2007
- id: harrison1978
  title: Hedonic housing prices and the demand for clean air
  author: 
  - family: Harrison
    given: D
  - family: Rubinfeld
    given: DL
  volume: 5
  publisher: Journal of Environmental Economics and Management
  type: article-journal
  issued: 
   year: 1978
- id: breiman2001
  title: Random forests
  author:
  - family: Breiman
    given: L
  volume: 45
  publisher: Machine Learning
  type: article-journal
  issued:
   year: 2001
- id: friedman2001
  title: "Greedy function approximation: a gradient boosting machine"
  author: 
  - family: Friedman
    given: JH
  publisher: The Annals of Applied Statistics
  type: article-journal
  issued:
   year: 2001
- id: zou2005
  title: Regularization and variable selection via the elastic net
  author:
  - family: Zou
    given: H
  - family: Hastie
    given: TJ
  publisher: "Journal of the Royal Statistical Society: Series B (Statistical Methodology)"
  type: article-journal
  issued:
   year: 2005
---

```{r, echo = FALSE, include = FALSE}
library(knitr)
opts_knit$set(cache = FALSE, verbose = TRUE, global.par = TRUE)
```

```{r echo = FALSE}
par(mar = c(5, 12, 4, 2) + 0.1)
```
Often when working with data we attempt to estimate the conditional mean of the outcome $Y$ given features $X$, defined as $\mu_P(x) = E_P(Y \mid X = x)$. 

There are many tools for estimating this conditional mean. We might choose a classical parametric tool such as linear regression. We might also want to be model-agnostic and use a more nonparametric approach to estimate the conditional mean. However, 

- This involves using some nonparametric smoothing technique, which requires: (1) choosing a technique, and (2) selecting tuning parameters
- Naive optimal tuning balances out the bias and variance of the smoothing estimator. Is this the correct trade-off for estimating the conditional mean?

Once we have a good estimate of the conditional mean, it is often of scientific interest to understand which features contribute the most to the variation in $\mu_P$. Specifically, we might consider \[\mu_{P, j}(x) = E_P(Y \mid X_{(-j)} = x_{(-j)}),\] where $X_{(-j)}$ is the vector with the $j$th element, or some group of elements, removed. By comparing $\mu_{P, j}$ to $\mu_P$ we can evaluate the importance of the $j$th element (or group of elements).

Assume that our data are generated according to the mechanism $P_0$. We can then define a nonparametric measure of variable importance, \[\psi_{0, j} = \frac{\int [\mu_{P_0}(x) - \mu_{P_0, j}(x)]^2dP_0(x)}{\text{Var}_{P_0}(Y)},\] which is the proportion of the variability in the outcome explained by including $X_j$ in our chosen estimation technique. 

This document introduces you to the basic tools in npvi and how to apply them to a dataset. I will explore the two different ways of obtaining variable estimates using npvi:

1. You only specify a *library* of candidate estimators for the conditional means $\mu_{P_0}$ and $\mu_{P_0, j}$; you allow npvi to obtain the optimal estimates of these quantities using the `SuperLearner` [@vanderlaan2007], and use these estimates to obtain variable importance estimates
2. You have a favorite estimator for the conditional means; you simply want npvi to obtain variable importance estimates using this estimator

## A look at the Boston housing study data

Throughout this document I will use the Boston housing study data [@harrison1978], freely available from the `MASS` package. Use `?Boston` to see documentation for these data.

```{r}
## load the library, view the data
library(MASS)
data(Boston)
head(Boston)
```

In addition to the median house value `medv`, the outcome of interest, there are measurements on four groups of variables. First are accessibility features: the weighted distances to five employment centers in the Boston region `dis`, where housing prices are expected to increase with decreased distance to employment centers; and an index of accessibility to radial highways `rad`, where housing prices are expected to increase with increased access to highways. Second are neighborhood features: the proportion of black residents in the population `black`; the proportion of population that is lower status `lstat`, which denotes adults without some high school education and male workers classified as laborers; the crime rate `crim`; the proportion of a town's residential land zoned for lots greater than 25,000 square feet `zn`; the proportion of nonretail business acres per town `indus`; the full value property tax rate `tax`; the pupil-teacher ratio  by school district `ptratio`; and an indicator of whether the tract of land borders the Charles River `chas`. The third group are structural features: the average number of rooms in owner units `rm`; and the proportion of owner units built prior to 1940 `age`. The final group is the nitrogen oxide concentration `nox`. 

Since there are 13 features and four groups, it is of interest to determine variable importance both for the 13 features separately and for the four groups of features.

## Building a library of learners

Suppose that I prefer to use generalized additive models [@hastie1990] to estimate $\mu_{P_0}$ and $\mu_{P_0, j}$, so I am planning on using the `mgcv` package. Suppose that you prefer to use the elastic net [@zou2005], and are planning to use the `glmnet` package. 

The choice of either method is somewhat subjective, and I also will have to use a technique like cross-validation to determine an optimal tuning parameter in each case. It is also possible that neither additive models nor the elastic net will do a good job estimating the true conditional means! This motivates using `SuperLearner` to allow the data to determine the optimal combination of *base learners* from a *library* that I define. These base learners are a combination of different methods (e.g. generalized additive models and elastic net) and instances of the same method with different tuning parameter values (e.g. additive models with 3 and 4 degrees of freedom).

For instance, my library could include additive models, elastic net , random forests [@breiman2001], and gradient boosted trees [@friedman2001] as follows:

```{r}
## load the library
library(SuperLearner)

## create a function for boosted stumps
SL.gbm.1 <- function(..., interaction.depth = 1) SL.gbm(..., interaction.depth = interaction.depth)

## create GAMs with different degrees of freedom
SL.gam.3 <- function(..., deg.gam = 3) SL.gam(..., deg.gam = deg.gam)
SL.gam.4 <- function(..., deg.gam = 4) SL.gam(..., deg.gam = deg.gam)
SL.gam.5 <- function(..., deg.gam = 5) SL.gam(..., deg.gam = deg.gam)

## add more levels of alpha for glmnet
create.SL.glmnet <- function(alpha = c(0.25, 0.5, 0.75)) {
  for (mm in seq(length(alpha))) {
    eval(parse(file = "", text = paste('SL.glmnet.', alpha[mm], '<- function(..., alpha = ', alpha[mm], ') SL.glmnet(..., alpha = alpha)', sep = '')), envir = .GlobalEnv)
  }
  invisible(TRUE)
}
create.SL.glmnet()

## add tuning parameters for randomForest
create.SL.randomForest <- function(tune = list(mtry = c(1, 5, 7), nodesize = c(1, 5, 10))) {
  tuneGrid <- expand.grid(tune, stringsAsFactors = FALSE)
  for (mm in seq(nrow(tuneGrid))) {
    eval(parse(file = "", text = paste("SL.randomForest.", mm, "<- function(..., mtry = ", tuneGrid[mm, 1], ", nodesize = ", tuneGrid[mm, 2], ") SL.randomForest(..., mtry = mtry, nodesize = nodesize)", sep = "")), envir = .GlobalEnv)
  }
  invisible(TRUE)
}
create.SL.randomForest()

## create the library
learners <- c("SL.gam", "SL.gam.3", "SL.gam.4", "SL.gam.5",
              "SL.glmnet", "SL.glmnet.0.25", "SL.glmnet.0.5", "SL.glmnet.0.75",
              "SL.randomForest", "SL.randomForest.1", "SL.randomForest.2", "SL.randomForest.3",
              "SL.randomForest.4", "SL.randomForest.5", "SL.randomForest.6", "SL.randomForest.7",
              "SL.randomForest.8", "SL.randomForest.9",
              "SL.gbm.1")
```

Now that I have created the library of learners, I can move on to estimating variable importance.

## Estimating variable importance for a single variable

The main function in the npvi package is the `vim()` function. There are three main arguments to `vim()`:

- `f1` and `f2`, which specify whether or not I need to estimate the conditional means 
- `data`, which supplies data to the function
- `j`, which determines the feature I want to estimate variable importance for

There are three ways to specify `f1` and `f2`:

1. Use formula notation and supply a library of learners (e.g. `learners` above)
2. Supply fitted values for our estimate of $\mu_{P_0}$ but supply a library of learners for estimating $\mu_{P_0, j}$
3. Supply fitted values for both estimates

I will illustrate each of these choices in order below, but in general I use (1) to estimate variable importance for the first feature in a given dataset and (2) or (3) to estimate variable importance for subsequent features in the same dataset. Method (3) allows the most flexibility, and all time-intensive computation occurs before calling `vim()`; however, this involves more work on your end. 

Suppose that the first feature that I want to estimate variable importance for is nitrogen oxide, `nox`. Since this is the first feature, say I choose (1) above. Then supplying `vim()` with 

- `f1 = y ~ x`
- `f2 = fit ~ x`
- `data = Boston`
- `j = 5` 

means that: 

- I want to use `SuperLearner()` to estimate the conditional mean $\mu_{P_0}$
- I want to use the two-step procedure outlined in Williamson 2017 (CITE) to estimate the conditional mean $\mu_{P_0, j}$
- I want to estimate variable importance for the fifth column of `Boston`, which is `nox`

The call to `vim()` looks like this:
```{r, eval = FALSE}
## load the library
library(npvi)

## first re-order the data so that the outcome is in the first column
Boston2 <- Boston[, 1:13]
Boston3 <- cbind(medv = Boston$medv, Boston2)

## now estimate variable importance
vim(f1 = y ~ x, f2 = fit ~ x, data = Boston3, y = Boston3[, 1], 
    j = 5, SL.library = learners)
```

While this is the preferred method for estimating variable importance, using a large library of learners may cause the function to take time to run. Usually this is okay --- in general, you took a long time to collect the data so letting an algorithm run for a few hours should not be an issue. 

However, for the sake of illustration, I can estimate varibable importance for nitrogen oxide only using only two base learners as follows:
```{r}
## load the library
library(npvi)

## first re-order the data so that the outcome is in the first column
Boston2 <- Boston[, 1:13]
Boston3 <- cbind(medv = Boston$medv, Boston2)

## new learners library, with only one learner for illustration only
learners.2 <- c("SL.gam", "SL.glmnet")

## now estimate variable importance
nox.vim <- vim(f1 = y ~ x, f2 = fit ~ x, data = Boston3, y = Boston3$medv, j = 5, SL.library = learners.2)
```

This code takes approximately 11 seconds to run on a (not very fast) PC. Under the hood, `vim()` fits the `SuperLearner()` function with the specified library, and then returns fitted values and variable importance estimates. This is most suitable for estimating variable importance for the first feature on a given dataset. I can display these estimates:

```{r}
nox.vim
```

The object returned by `vim()` also contains fitted values from using `SuperLearner()`; I access these using `$full.fit` and `$red.fit`. For example,

```{r}
head(nox.vim$full.fit)
head(nox.vim$red.fit)
```

I said earlier that I want to obtain estimates of all individual features in these data, so let's choose average number of rooms (`rm`) next. Now that I have estimated variable importance for nitrogen oxide, the `full.fit` object contains our estimate of $\mu_{P_0}$. Since I have spent the time to estimate this using `SuperLearner()`, there is no reason to estimate this function again. This leads me to choose (2) above, since I have already estimated variable importance on one feature in this dataset. Using the small learners library (again only for illustration) yields

```{r}
## specify that full.fit doesn't change
full.fit <- nox.vim$full.fit

## estimate variable importance for the average number of rooms
rm.vim <- vim(full.fit, f2 = fit ~ x, data = Boston3, y = Boston3$medv,
              j = 6, SL.library = learners.2)

rm.vim
```

This takes approximately 5 seconds --- now rather than estimating both conditional means, I am only estimating one.

If I choose (3), then I have to use a single method from the library, or call `SuperLearner()` ourselves, prior to estimating variable importance. Then `vim()` returns variable importance estimates based on these fitted values. For example, let's estimate variable importance for the distance to radial highways using this approach.

```{r}
## set up the data
y <- Boston$medv
x <- Boston[, -c(8, 14)] # this removes dis and medv

## fit a GAM and glmnet using SuperLearner, using the two-step estimating procedure
reduced.mod <- SuperLearner(Y = full.fit, X = x, SL.library = learners.2)
reduced.fit <- predict(reduced.mod)$pred
## this takes 2 seconds

## estimate variable importance
dis.vim <- vim(full.fit, reduced.fit, y = Boston3$medv, j = 8)
```

It is important to note that if I use the same learners library, then approaches (2) and (3) are equivalent. 

I can obtain estimates for the remaining individual features in the same way (again using only two base learners for illustration):
```{r}
crim.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 1, SL.library = learners.2)
zn.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 2, SL.library = learners.2)
indus.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 3, SL.library = learners.2)
chas.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 4, SL.library = learners.2)
age.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 7, SL.library = learners.2)
rad.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 9, SL.library = learners.2)
tax.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 10, SL.library = learners.2)
ptratio.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 11, SL.library = learners.2)
black.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 12, SL.library = learners.2)
lstat.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = 13, SL.library = learners.2)
```

Now that I have estimates of each of individual feature's variable importance, I can view them all simultaneously by plotting:
```{r, fig.width = 8.5, fig.height = 8}
## combine the objects together
ests <- combine(crim.vim, zn.vim, indus.vim, chas.vim,
                nox.vim, rm.vim, age.vim, dis.vim, rad.vim,
                tax.vim, ptratio.vim, black.vim, lstat.vim)

## create a vector of names; must be in the same order as the
## mat object in ests
nms <- c("Prop. lower status", "Avg. num. rooms", "Pupil-teacher ratio", "Nitrogen oxide", "Distance", "Crime", "Access to radial hwys", "Property tax rate", "Charles riv.", "Prop. black", "Prop. business", "Prop. large zoned", "Age")

## plot
plot(ests, nms, pch = 16, ylab = "", xlab = "Estimate", main = "Estimated variable importance for individual features", xlim = c(0, 0.2), axes = FALSE)
```

## Estimating variable importance for a group of variables

Now that I have estimated variable importance for each of the individual features, I can estimate variable importance for each of the groups that I mentioned above: accessibility features, structural features, nitrogen oxide, and neighborhood features.

The only difference between estimating variable importance for a group of features rather than an individual feature is that now I specify a vector for `j`; I can use any of the options listed in the previous section to compute these estimates.

```{r, fig.width = 8.5, fig.height = 8}
## get the estimates
structure.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = c(6, 7), SL.library = learners.2)
access.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = c(8, 9), SL.library = learners.2)
neigh.vim <- vim(full.fit, fit ~ x, data = Boston3, y = Boston3$medv,
                j = c(1, 2, 3, 4, 10, 11, 12, 13), SL.library = learners.2)

## combine and plot
groups <- combine(structure.vim, access.vim, neigh.vim, nox.vim)
nms.2 <- c("Neighborhood", "Structure", "Accessibility", "Nitrogen oxide")
plot(groups, nms.2, pch = 16, ylab = "", xlab = "Estimate", main = "Estimated variable importance for groups", xlim = c(0, 0.5), axes = FALSE)
```

## References
